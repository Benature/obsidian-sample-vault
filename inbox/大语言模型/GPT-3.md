---
beginDate: 2020-06-11
wiki: https://zh.wikipedia.org/wiki/GPT-3
tags:
  - "#timeline"
company: "[[OpenAI]]"
aliases:
  - Generative Pre-trained Transformer 3
---
GPT-3 是一个自回归语言模型，目的是为了使用深度学习生成人类可以理解的自然语言。GPT-3 是由在旧金山的人工智能公司 OpenAI 训练与开发，模型设计基于谷歌开发的 Transformer 语言模型。GPT-3 的神经网络包含 1750 亿个参数，需要 700 GB 来存储，为有史以来参数最多的神经网络模型。该模型在许多任务上展示了强大的零样本和少样本的能力。

OpenAI 于 2020 年 5 月发表 GPT-3 的论文，在次月为少量公司与开发人团发布应用程序接口的测试版。微软在 2020 年 9 月 22 日宣布获取了 GPT-3 的独家授权。

GPT-3 被认为可写出人类无法与电脑区别的文章与字符串，GPT-3 原始论文的作者们警告了 GPT-3 有可能对于社会的负面影响，比如利用制造假新闻的可能性。英国《卫报》即使用 GPT-3 生成了一个关于人工智能对人类无威胁的评论专栏。李开复称卷积神经网络与 GPT-3 为人工智能重要的改善，两者皆是模型加海量数据的成果。